{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599152942196",
   "display_name": "Python 3.7.7 64-bit ('tf-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimiento con RNNs y word *embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con *embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape de los datos X de entrenamiento:  (25000,)\nShape de los datos Y de entrenamiento:  (25000,)\nShape de los datos X de validación:  (25000,)\nShape de los datos Y de validación:  (25000,)\n"
    }
   ],
   "source": [
    "MAX_WORDS = 20000   # Número máximo de palabras para cargar y ENTRENAR el embedding\n",
    "\n",
    "# Cargar datos de IMDB\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=MAX_WORDS, start_char=None)\n",
    "\n",
    "print('Shape de los datos X de entrenamiento: ', x_train.shape)\n",
    "print('Shape de los datos Y de entrenamiento: ', y_train.shape)\n",
    "print('Shape de los datos X de validación: ', x_val.shape)\n",
    "print('Shape de los datos Y de validación: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspección de un ejemplo\n",
    "# x_train[0]\n",
    "# y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtener el diccionario de los reviews\n",
    "word_index = keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape de los datos X de entrenamiento:  (25000, 200)\nShape de los datos X de validación:  (25000, 200)\n"
    }
   ],
   "source": [
    "# Vamos a preprocesar los datos, en secuencias de tamaño fijo de largo 200\n",
    "MAX_SEQ_LENGTH = 200\n",
    "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQ_LENGTH)\n",
    "x_val_padded = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "print('Shape de los datos X de entrenamiento: ', x_train_padded.shape)\n",
    "print('Shape de los datos X de validación: ', x_val_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 200, 128)          2560000   \n_________________________________________________________________\nlstm (LSTM)                  (None, 200, 64)           49408     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 64)                33024     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 65        \n=================================================================\nTotal params: 2,642,497\nTrainable params: 2,642,497\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Definición del modelo de sentimiento\n",
    "WORD_REPR_DIM = 128 # Dimensión del vector que representa cada palabra\n",
    "\n",
    "sentiment_model = keras.models.Sequential([\n",
    "    # Capa de embeddings para trasladar cada número en las secuencias a un vector \n",
    "    # de representación de cada palabra\n",
    "    keras.layers.Embedding(MAX_WORDS, WORD_REPR_DIM, input_length=200),\n",
    "    # Unidades LSTM para aprender los patrones en las secuencias que determinan un\n",
    "    # comentario positivo o negativo\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    keras.layers.LSTM(64),\n",
    "    # Unidad de salida, clasifica si el comentario es positivo o negativo\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/2\n25000/25000 [==============================] - 50s 2ms/sample - loss: 0.3982 - accuracy: 0.8194 - val_loss: 0.3048 - val_accuracy: 0.8695\nEpoch 2/2\n25000/25000 [==============================] - 46s 2ms/sample - loss: 0.2145 - accuracy: 0.9190 - val_loss: 0.3207 - val_accuracy: 0.8663\n"
    }
   ],
   "source": [
    "# Compilación y entrenamiento del modelo\n",
    "sentiment_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = sentiment_model.fit(x = x_train_padded, y = y_train, \\\n",
    "    batch_size=32, epochs=2, \\\n",
    "    validation_data = (x_val_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_review(review_str, sentiment_model): \n",
    "    # Obtener la secuencia de números de palabras\n",
    "    # 'this movie is awesome very exciting and romantic' -> [11, 17, 6, 1187, 52, 1124, 2, 728]\n",
    "    review_seq = [word_index[token] for token in review_str.split()]\n",
    "    # Hacer padding de la secuencia\n",
    "    # [11, 17, 6, 1187, 52, 1124, 2, 728] -> [0, 0, ..., 0, 11, 17, 6, 1187, 52, 1124, 2, 728]\n",
    "    review_seq_padded = keras.preprocessing.sequence.pad_sequences([review_seq], maxlen=MAX_SEQ_LENGTH)\n",
    "    # Obtener el sentimiento con el modelo y devolverlo\n",
    "    sentiment = sentiment_model(review_seq_padded)\n",
    "    return sentiment.numpy().flatten()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a utilizarlo para un ejemplo propio =P\n",
    "# Después de un preprocesamiento de signos de puntuación y exclamación \n",
    "# podríamos tener algo así: \n",
    "own_reviews = ['this movie is awesome very exciting', \\\n",
    "    'awful movie']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Review: \" this movie is awesome very exciting \" ->  0.6356226\nReview: \" awful movie \" ->  0.43795732\n"
    }
   ],
   "source": [
    "# Mostramos el punteo de sentimiento de los reviews\n",
    "for review in own_reviews: \n",
    "    print('Review: \"', review, '\" -> ', sentiment_review(review, sentiment_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Entrenamiento del modelo con *transfer learning* de *embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 20_000   # Número máximo de palabras para cargar y UTILIZAR el embedding\n",
    "EMBEDDING_DIM = 200   # Dimensiones de los vectores de representación de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo Stanford Glove embeddings\n",
    "word2vec_file = '../../Glove/glove.twitter.27B.200d.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_file, limit=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape de los datos X de entrenamiento:  (25000,)\nShape de los datos Y de entrenamiento:  (25000,)\nShape de los datos X de validación:  (25000,)\nShape de los datos Y de validación:  (25000,)\n"
    }
   ],
   "source": [
    "# Cargar nuevamente datos de IMDB, con más palabras\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=MAX_WORDS, start_char=None)\n",
    "\n",
    "print('Shape de los datos X de entrenamiento: ', x_train.shape)\n",
    "print('Shape de los datos Y de entrenamiento: ', y_train.shape)\n",
    "print('Shape de los datos X de validación: ', x_val.shape)\n",
    "print('Shape de los datos Y de validación: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el diccionario de palabras del dataset, para convertir a los índices del embedding\n",
    "#   mapea: palabra -> # índice en el dataset de keras\n",
    "word_index = keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")\n",
    "\n",
    "# Obtener el diccionario inverso, de índices a palabras. \n",
    "#   mapea: índice en el dataset de keras -> palabra\n",
    "# index_word = {}\n",
    "# for keys, index_imdb in word_index.items():\n",
    "#     index_word[index_imdb] = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos algunas entradas del diccionario\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tamaño de la matriz de embeddings:  (88584, 200)\n"
    }
   ],
   "source": [
    "# Definimos nuestra matriz de embeddings para el vocabulario\n",
    "VOCAB_SIZE = len(word_index)\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "# Llenar la matriz de embeddings con el modelo glove preentrenado\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        # Si la palabra está en el modelo, se guarda en la fila de la matriz\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "    except KeyError:\n",
    "        # Las palabras no encontradas en el modelo glove serán cero\n",
    "        continue\n",
    "\n",
    "print('Tamaño de la matriz de embeddings: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n         0.      ],\n       [ 0.4935  ,  0.35698 ,  0.66068 , ...,  0.17706 , -0.53695 ,\n        -0.29699 ],\n       [ 0.026598, -0.26277 ,  0.05302 , ...,  0.1305  , -0.22232 ,\n         0.26772 ],\n       ...,\n       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n         0.      ],\n       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n         0.      ],\n       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n         0.      ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Mostramos algunas entradas de la matriz de embeddings\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Shape de los datos X de entrenamiento:  (25000, 200)\nShape de los datos X de validación:  (25000, 200)\n"
    }
   ],
   "source": [
    "# Vamos a preprocesar los datos, en secuencias de tamaño fijo de largo 200\n",
    "MAX_SEQ_LENGTH = 200\n",
    "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_SEQ_LENGTH)\n",
    "x_val_padded = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "print('Shape de los datos X de entrenamiento: ', x_train_padded.shape)\n",
    "print('Shape de los datos X de validación: ', x_val_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 200, 200)          17716800  \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 200, 64)           67840     \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 64)                33024     \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 17,817,729\nTrainable params: 100,929\nNon-trainable params: 17,716,800\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Definición del modelo de sentimiento con transfer learning\n",
    "\n",
    "sentiment_trlearn_model = keras.models.Sequential([\n",
    "    # Capa de embeddings para trasladar cada número en las secuencias a un vector \n",
    "    # de representación de cada palabra. \n",
    "    # Notar ahora que damos los pesos iniciales (preentrenados) y los fijamos para\n",
    "    # que no se modifiquen durante el entrenamiento\n",
    "    keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=200, \\\n",
    "        embeddings_initializer = keras.initializers.Constant(embedding_matrix), \\\n",
    "        trainable = False),\n",
    "    # Unidades LSTM para aprender los patrones en las secuencias que determinan un\n",
    "    # comentario positivo o negativo\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    keras.layers.LSTM(64),\n",
    "    # Unidad de salida, clasifica si el comentario es positivo o negativo\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compilar el modelo \n",
    "sentiment_trlearn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostrar un resumen\n",
    "sentiment_trlearn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/4\n25000/25000 [==============================] - 31s 1ms/sample - loss: 0.6261 - accuracy: 0.6332 - val_loss: 0.5274 - val_accuracy: 0.7416\nEpoch 2/4\n25000/25000 [==============================] - 28s 1ms/sample - loss: 0.4717 - accuracy: 0.7704 - val_loss: 0.4139 - val_accuracy: 0.8103\nEpoch 3/4\n25000/25000 [==============================] - 28s 1ms/sample - loss: 0.3781 - accuracy: 0.8278 - val_loss: 0.3759 - val_accuracy: 0.8347\nEpoch 4/4\n25000/25000 [==============================] - 28s 1ms/sample - loss: 0.3141 - accuracy: 0.8634 - val_loss: 0.3556 - val_accuracy: 0.8395\n"
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "history = sentiment_trlearn_model.fit(x = x_train_padded, y = y_train, \\\n",
    "    batch_size=32, epochs=4, \\\n",
    "    validation_data = (x_val_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a utilizarlo para un ejemplo propio =P\n",
    "# Después de un preprocesamiento de signos de puntuación y exclamación \n",
    "# podríamos tener algo así: \n",
    "own_reviews = ['this movie is awesome very exciting', \\\n",
    "    'awful movie']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Review: \" this movie is awesome very exciting \" ->  0.6902058\nReview: \" awful movie \" ->  0.39027804\n"
    }
   ],
   "source": [
    "# Mostramos el punteo de sentimiento de los reviews\n",
    "for review in own_reviews: \n",
    "    print('Review: \"', review, '\" -> ', sentiment_review(review, sentiment_trlearn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}